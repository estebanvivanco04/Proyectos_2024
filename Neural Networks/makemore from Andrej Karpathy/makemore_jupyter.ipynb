{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27,27), dtype = torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i , s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
       "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
       "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdim= True)\n",
    "# ahora estan todas las filas normalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    ix = 0\n",
    "    word = []\n",
    "    while True:\n",
    "        #p = N[ix].float()\n",
    "        #p = p / p.sum()\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement= True, generator = g).item()\n",
    "        word.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e: 0.0478 -3.0410\n",
      "em: 0.0377 -3.2793\n",
      "mm: 0.0253 -3.6753\n",
      "ma: 0.3885 -0.9454\n",
      "a.: 0.1958 -1.6305\n",
      ".o: 0.0123 -4.3965\n",
      "ol: 0.0779 -2.5526\n",
      "li: 0.1774 -1.7293\n",
      "iv: 0.0152 -4.1845\n",
      "vi: 0.3508 -1.0476\n",
      "ia: 0.1380 -1.9807\n",
      "a.: 0.1958 -1.6305\n",
      ".a: 0.1376 -1.9835\n",
      "av: 0.0246 -3.7041\n",
      "va: 0.2473 -1.3971\n",
      "a.: 0.1958 -1.6305\n",
      ".i: 0.0185 -3.9919\n",
      "is: 0.0743 -2.5998\n",
      "sa: 0.1478 -1.9119\n",
      "ab: 0.0160 -4.1363\n",
      "be: 0.2455 -1.4044\n",
      "el: 0.1589 -1.8396\n",
      "ll: 0.0962 -2.3408\n",
      "la: 0.1876 -1.6733\n",
      "a.: 0.1958 -1.6305\n",
      ".s: 0.0641 -2.7468\n",
      "so: 0.0654 -2.7270\n",
      "op: 0.0121 -4.4180\n",
      "ph: 0.1947 -1.6364\n",
      "hi: 0.0955 -2.3485\n",
      "ia: 0.1380 -1.9807\n",
      "a.: 0.1958 -1.6305\n",
      ".c: 0.0481 -3.0339\n",
      "ch: 0.1869 -1.6774\n",
      "ha: 0.2937 -1.2251\n",
      "ar: 0.0963 -2.3405\n",
      "rl: 0.0325 -3.4256\n",
      "lo: 0.0496 -3.0047\n",
      "ot: 0.0149 -4.2032\n",
      "tt: 0.0670 -2.7031\n",
      "te: 0.1281 -2.0549\n",
      "e.: 0.1948 -1.6357\n",
      ".m: 0.0792 -2.5358\n",
      "mi: 0.1885 -1.6687\n",
      "ia: 0.1380 -1.9807\n",
      "a.: 0.1958 -1.6305\n",
      ".a: 0.1376 -1.9835\n",
      "am: 0.0482 -3.0321\n",
      "me: 0.1228 -2.0971\n",
      "el: 0.1589 -1.8396\n",
      "li: 0.1774 -1.7293\n",
      "ia: 0.1380 -1.9807\n",
      "a.: 0.1958 -1.6305\n",
      ".h: 0.0273 -3.6011\n",
      "ha: 0.2937 -1.2251\n",
      "ar: 0.0963 -2.3405\n",
      "rp: 0.0012 -6.7434\n",
      "pe: 0.1880 -1.6711\n",
      "er: 0.0958 -2.3455\n",
      "r.: 0.1083 -2.2231\n",
      ".e: 0.0478 -3.0410\n",
      "ev: 0.0227 -3.7859\n",
      "ve: 0.2188 -1.5194\n",
      "el: 0.1589 -1.8396\n",
      "ly: 0.1136 -2.1749\n",
      "yn: 0.1864 -1.6800\n",
      "n.: 0.3685 -0.9982\n",
      "log_likelihood=tensor(-160.4268)\n",
      "nll=tensor(160.4268)\n",
      "2.3944296836853027\n"
     ]
    }
   ],
   "source": [
    "#ahora vamos a ver que tan bueno es el modelo, a partir de los nombres originales.\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words[:10]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip (chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}') # la calidad del modelo, cuanto mas cerca de cero mejor.\n",
    "print(f'{nll/n}') #es la calidad del modelo average\n",
    "\n",
    "#¿que hace este codigo? primero que va por toda la grilla de probabilidades(que letra es probable que le siga a otra)\n",
    "# despues con esa probabilidad, de agrega log, y luego ocn log_likelihood suma todas esas probabilidades\n",
    "# y como la suma de los log, es lo mismo que la multiplicacion de su input, osea que log(a) + log(b) + log(c) = log(a*b*C)\n",
    "# acordarse que a b y c son probabilidades, la log_likelihood que la multiplicacion de todas esas probabiliadaes.\n",
    "\n",
    "# nada mas que su suma al final se le aplica un log por conveniencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL : maximize likelihood of the data w.r.t model parameters (statistical modeling)\n",
    "# equivalent to maximizeing the log likelihood (because log in monotonic) monotonic is a funtion that preserves the given order\n",
    "# equivalent to minimizing the negative log likelihood\n",
    "# equivalent to minimizing the average negative log likelihood\n",
    "\n",
    "# acordarse que log(a) + log(b) + log(c) = log(a*b*b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuestra red neuronal, va a recibir un caracter, y va a devolver la probabilidad del SIGUIENTE caracter.\n",
    "# y vamos a usar la loss-funtion, osea, nos vamos a basar en -log_likelihood, conla loss-funtion, vamos a buscar un minimo\n",
    "# y para buscar un minimo vamos a jugar con los weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.1192e-05, 1.3759e-01, 4.0767e-02, 4.8129e-02, 5.2745e-02, 4.7785e-02,\n",
       "        1.3038e-02, 2.0898e-02, 2.7293e-02, 1.8465e-02, 7.5577e-02, 9.2452e-02,\n",
       "        4.9064e-02, 7.9195e-02, 3.5777e-02, 1.2321e-02, 1.6095e-02, 2.9008e-03,\n",
       "        5.1154e-02, 6.4130e-02, 4.0830e-02, 2.4641e-03, 1.1759e-02, 9.6070e-03,\n",
       "        4.2109e-03, 1.6719e-02, 2.9008e-02])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the training set of all the bigrams\n",
    "\n",
    "xs, ys = [], [] # vamos a hacer dos listas, 'xs' son los inputs, y 'ys' son los objetivos\n",
    "# en el primero van a estar los caracteres, y en el segundo la prediccion\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1,ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "\n",
    "# y ahora nos conviene usar tensores\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "yx = torch.tensor(ys)\n",
    "\n",
    "# entonces lo que hace esto es lo mismo que hacíamos antes, darle un valor de 0 a 26 a las letras, pero esta vez usamos tensores\n",
    "# luego usamos la inteligente funcion zip, para ordenar la palabra de forma que sea un bigram con\n",
    "# .esteban \n",
    "# esteban. entonces zip, va a ir agarrando (., e), (e, s) (s,t) y así hasta (n , .)\n",
    "type(xs[0].item())\n",
    "P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora vamos a usar una funtion llamada ONE HOT, la cual para un numero, crea un vector, que tiene longitud numero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes =27).float() # no se puede usar dtype =float32 porque en la especificacion de torch no deja.\n",
    "# esto lo que haces es hacer una grilla, donde las filas son las posiciones y estan llenas de 0 exepto en la columna donde esta el valor de la posicion.\n",
    " # en la grilla, la posicion [2,13] es un 1, ya que en la palabra emma, la letra m xtoi(m) = 13\n",
    "# hay que tener mucho cuidado con los data types de xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3984, -0.7719, -0.5301, -0.3948, -1.5728, -1.1998,  1.1549,  1.3326,\n",
       "          0.3140,  0.4446, -0.4605, -1.5793,  0.2157,  1.7033,  1.9903, -0.5601,\n",
       "         -1.3735, -1.4781,  0.8445,  0.2813, -0.9190, -0.2010,  0.5124,  1.0930,\n",
       "         -2.1139, -0.2706, -1.2021],\n",
       "        [-0.7743,  2.4054, -1.0919, -0.1741, -0.6260,  0.8478, -1.1761, -0.0519,\n",
       "         -1.2176,  0.6090, -0.8973,  0.9077,  1.5546, -0.1561,  0.2285, -1.3915,\n",
       "          1.1705, -1.7693, -0.0811, -0.5382,  0.6206, -0.5555,  0.5031,  0.3340,\n",
       "          1.6046, -0.5954,  0.2818],\n",
       "        [ 1.6366,  0.2133, -0.9695, -0.2006,  0.7623, -0.1863, -0.2994, -0.9337,\n",
       "          0.0239,  0.0599,  0.2794, -1.4746, -0.4258,  0.4068,  0.6020,  0.1340,\n",
       "         -0.2579,  0.6377,  0.0125, -1.4132,  1.4148,  0.8470,  1.6648, -0.6872,\n",
       "          0.3984, -0.8069, -1.0935],\n",
       "        [ 1.6366,  0.2133, -0.9695, -0.2006,  0.7623, -0.1863, -0.2994, -0.9337,\n",
       "          0.0239,  0.0599,  0.2794, -1.4746, -0.4258,  0.4068,  0.6020,  0.1340,\n",
       "         -0.2579,  0.6377,  0.0125, -1.4132,  1.4148,  0.8470,  1.6648, -0.6872,\n",
       "          0.3984, -0.8069, -1.0935],\n",
       "        [-0.4341,  1.3216, -0.3921,  1.7712, -0.9348,  1.0811, -0.4859,  1.6906,\n",
       "         -0.3748,  1.8114,  0.6256, -0.4296,  1.3997, -0.3667, -1.3195, -1.1512,\n",
       "          0.5612,  0.2790,  0.9953,  1.3594,  0.3416, -0.8045, -0.1710, -0.8106,\n",
       "          0.3283, -0.6139,  0.5515]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27,27)) # crea un tensor con numeros random de la normal distribution, muchos van a ser 0 y algunos van a llegar a ser 3\n",
    "xenc @ W # el @ es la multiplicacion de matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc @ W) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
